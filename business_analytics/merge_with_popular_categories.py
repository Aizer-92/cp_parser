#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –î–ê–ù–ù–´–• –° –ü–û–ü–£–õ–Ø–†–ù–´–ú–ò –ö–ê–¢–ï–ì–û–†–ò–Ø–ú–ò
–î–æ–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –∏–∑ CSV —Ñ–∞–π–ª–∞
"""

import sqlite3
import pandas as pd
import numpy as np
from pathlib import Path
from business_final_analyzer import BusinessFinalAnalyzer
import re

def normalize_category_name(name):
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è"""
    if pd.isna(name) or name == '':
        return ''
    
    name = str(name).lower().strip()
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
    name = re.sub(r'[^\w\s]', ' ', name)
    name = re.sub(r'\s+', ' ', name)
    
    # –ó–∞–º–µ–Ω—ã –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è
    replacements = {
        '–∫–∞—Ä—Ç—Ö–æ–ª–¥–µ—Ä': '–∫–∞—Ä—Ç—Ö–æ–ª–¥–µ—Ä',
        '–∫–∞—Ä–¥—Ö–æ–ª–¥–µ—Ä': '–∫–∞—Ä—Ç—Ö–æ–ª–¥–µ—Ä', 
        '—Ä—é–∫–∑–∞–∫': '—Ä—é–∫–∑–∞–∫',
        '—Å—É–º–∫–∞': '—Å—É–º–∫–∞',
        '—à–æ–ø–ø–µ—Ä': '—Å—É–º–∫–∞',
        '–æ—Ä–≥–∞–Ω–∞–π–∑–µ—Ä': '–æ—Ä–≥–∞–Ω–∞–π–∑–µ—Ä',
        '–∫–æ—Å–º–µ—Ç–∏—á–∫–∞': '–∫–æ—Å–º–µ—Ç–∏—á–∫–∞',
        '–ø–µ–Ω–∞–ª': '–ø–µ–Ω–∞–ª',
        '—Ç–µ—Ä–º–æ—Å—É–º–∫–∞': '—Ç–µ—Ä–º–æ—Å—É–º–∫–∞',
        '–∞–≤–æ—Å—å–∫–∞': '–∞–≤–æ—Å—å–∫–∞',
        '—Å—Ç–∏–∫–µ—Ä': '–Ω–∞–∫–ª–µ–π–∫–∞',
        '–Ω–∞–∫–ª–µ–π–∫–∞': '–Ω–∞–∫–ª–µ–π–∫–∞',
        '–µ–∂–µ–¥–Ω–µ–≤–Ω–∏–∫': '–µ–∂–µ–¥–Ω–µ–≤–Ω–∏–∫',
        '–±–ª–æ–∫–Ω–æ—Ç': '–±–ª–æ–∫–Ω–æ—Ç',
        '–Ω–æ—Å–∫–∏': '–Ω–æ—Å–∫–∏',
        '—à–∞—Ä—Ñ': '—à–∞—Ä—Ñ',
        '–ø–ª–∞—Ç–æ–∫': '—à–∞—Ä—Ñ',
        '–∫–µ–ø–∫–∞': '–∫–µ–ø–∫–∞',
        '–±–µ–π—Å–±–æ–ª–∫–∞': '–∫–µ–ø–∫–∞',
        '—Ñ—É—Ç–±–æ–ª–∫–∞': '—Ñ—É—Ç–±–æ–ª–∫–∞',
        '—Ö—É–¥–∏': '—Ö—É–¥–∏',
        '—Ç–æ–ª—Å—Ç–æ–≤–∫–∞': '—Ö—É–¥–∏',
        '—à–∞–ø–∫–∞': '—à–∞–ø–∫–∞',
        '–ø–ª–µ–¥': '–ø–ª–µ–¥',
        '–ø–æ–ª–æ—Ç–µ–Ω—Ü–µ': '–ø–æ–ª–æ—Ç–µ–Ω—Ü–µ',
        '–ª–∞–Ω—á–±–æ–∫—Å': '–ª–∞–Ω—á–±–æ–∫—Å',
        '–±—É—Ç—ã–ª–∫–∞': '–±—É—Ç—ã–ª–∫–∞',
        '—Å—Ç–∞–∫–∞–Ω': '–±—É—Ç—ã–ª–∫–∞',
        '—Ç–µ—Ä–º–æ—Å': '—Ç–µ—Ä–º–æ—Å',
        '—Ç–µ—Ä–º–æ–∫—Ä—É–∂–∫–∞': '—Ç–µ—Ä–º–æ—Å',
        '—Ç–µ—Ä–º–æ—Å—Ç–∞–∫–∞–Ω': '—Ç–µ—Ä–º–æ—Å',
        '–ø–æ—Å—É–¥–∞': '–ø–æ—Å—É–¥–∞',
        '—Ç–∞—Ä–µ–ª–∫–∞': '–ø–æ—Å—É–¥–∞',
        '–∫—Ä—É–∂–∫–∞': '–∫—Ä—É–∂–∫–∞',
        '–±–ª–µ–Ω–¥–µ—Ä': '–±–ª–µ–Ω–¥–µ—Ä',
        '–≥—Ä–µ–ª–∫–∞': '–≥—Ä–µ–ª–∫–∞',
        '–º—ã—à—å': '–º—ã—à—å',
        '–º–∞—Å—Å–∞–∂–µ—Ä': '–º–∞—Å—Å–∞–∂–µ—Ä',
        '–∫–æ–ª–æ–Ω–∫–∞': '–∫–æ–ª–æ–Ω–∫–∞',
        '–Ω–∞—É—à–Ω–∏–∫': '–Ω–∞—É—à–Ω–∏–∫',
        '–ø–∞—É—ç—Ä–±–∞–Ω–∫': '–ø–∞—É—ç—Ä–±–∞–Ω–∫',
        '–ø–æ–≤–µ—Ä–±–∞–Ω–∫': '–ø–∞—É—ç—Ä–±–∞–Ω–∫',
        '–¥–æ–∫—Å—Ç–∞–Ω—Ü–∏—è': '–¥–æ–∫—Å—Ç–∞–Ω—Ü–∏—è',
        '—É–≤–ª–∞–∂–Ω–∏—Ç–µ–ª—å': '—É–≤–ª–∞–∂–Ω–∏—Ç–µ–ª—å',
        '–ª–∞–º–ø–∞': '–ª–∞–º–ø–∞',
        '—Å–≤–µ—Ç–∏–ª—å–Ω–∏–∫': '–ª–∞–º–ø–∞',
        '–Ω–æ—á–Ω–∏–∫': '–ª–∞–º–ø–∞',
        '—Ñ–æ–Ω–∞—Ä–∏–∫': '—Ñ–æ–Ω–∞—Ä–∏–∫',
        '–∑–Ω–∞—á–æ–∫': '–∑–Ω–∞—á–æ–∫',
        '–±—Ä–µ–ª–æ–∫': '–±—Ä–µ–ª–æ–∫',
        '–º–∞–≥–Ω–∏—Ç': '–º–∞–≥–Ω–∏—Ç',
        '—Ñ–ª–µ—à–∫–∞': '—Ñ–ª–µ—à–∫–∞',
        '–∫–∞–±–µ–ª—å': '–∫–∞–±–µ–ª—å',
        '–ø—Ä–æ–≤–æ–¥': '–∫–∞–±–µ–ª—å',
        '—à–Ω—É—Ä–æ–∫': '—à–Ω—É—Ä–æ–∫',
        '–ª–∞–Ω—å—è—Ä–¥': '–ª–∞–Ω—å—è—Ä–¥',
        '—á–∞—Å—ã': '—á–∞—Å—ã',
        '–ø–æ–¥—É—à–∫–∞': '–ø–æ–¥—É—à–∫–∞',
        '–º–∞—Å–∫–∞': '–º–∞—Å–∫–∞',
        '–µ–ª–æ—á–Ω–∞—è –∏–≥—Ä—É—à–∫–∞': '–µ–ª–æ—á–Ω–∞—è –∏–≥—Ä—É—à–∫–∞',
        '—Å–Ω–µ–∂–Ω—ã–π —à–∞—Ä': '—Å–Ω–µ–∂–Ω—ã–π —à–∞—Ä',
        '–∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä': '–∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä',
        '–º—è–≥–∫–∞—è –∏–≥—Ä—É—à–∫–∞': '–∏–≥—Ä—É—à–∫–∞',
        '–ø–∞–∑–ª': '–ø–∞–∑–ª',
        '–≥–æ–ª–æ–≤–æ–ª–æ–º–∫–∞': '–≥–æ–ª–æ–≤–æ–ª–æ–º–∫–∞',
        '–∫—É–±–∏–∫ —Ä—É–±–∏–∫–∞': '–≥–æ–ª–æ–≤–æ–ª–æ–º–∫–∞',
        '—à–∞—à–∫–∏': '—à–∞—à–∫–∏',
        '—à–∞—Ö–º–∞—Ç—ã': '—à–∞—à–∫–∏',
        '–∫—Ä—É–≥': '–∫—Ä—É–≥',
        '—Ä–µ–∑–∏–Ω–∫–∞': '—Ä–µ–∑–∏–Ω–∫–∞',
        '—Å–∫–∞–∫–∞–ª–∫–∞': '—Å–∫–∞–∫–∞–ª–∫–∞',
        '–∑–æ–Ω—Ç': '–∑–æ–Ω—Ç',
        '–∫–æ—Ä–æ–±–∫–∞': '–∫–æ—Ä–æ–±–∫–∞',
        '–∞–≤—Ç–æ–≤–∏–∑–∏—Ç–∫–∞': '–∞–≤—Ç–æ–≤–∏–∑–∏—Ç–∫–∞',
        '–ø—Ä–æ–µ–∫—Ç–æ—Ä': '–ø—Ä–æ–µ–∫—Ç–æ—Ä',
        '—Ñ–æ—Ç–æ–∞–ø–ø–∞—Ä–∞—Ç': '—Ñ–æ—Ç–æ–∞–ø–ø–∞—Ä–∞—Ç',
        '—Ä—É—á–∫–∞': '—Ä—É—á–∫–∞',
        '—Ç–µ—Ç—Ä–∏—Å': '—Ç–µ—Ç—Ä–∏—Å',
        '–∫–∞—Ä–∞–Ω–¥–∞—à': '–∫–∞—Ä–∞–Ω–¥–∞—à',
        '–ø–æ–ø—Å–æ–∫–µ—Ç': '–ø–æ–ø—Å–æ–∫–µ—Ç',
        '–æ–±–ª–æ–∂–∫–∞': '–æ–±–ª–æ–∂–∫–∞',
        '—á–µ–º–æ–¥–∞–Ω': '—á–µ–º–æ–¥–∞–Ω',
        '—Å—Ç–∞—Ç—É—ç—Ç–∫–∞': '—Å—Ç–∞—Ç—É—ç—Ç–∫–∞',
        '–Ω–∞–≥—Ä–∞–¥–∞': '—Å—Ç–∞—Ç—É—ç—Ç–∫–∞',
        '–ø–∞–Ω–∞–º–∞': '–ø–∞–Ω–∞–º–∞',
        '–¥–µ—Ä–∂–∞—Ç–µ–ª—å': '–¥–µ—Ä–∂–∞—Ç–µ–ª—å',
        '–≥–∏—Ä–ª—è–Ω–¥–∞': '–≥–∏—Ä–ª—è–Ω–¥–∞',
        '—à–ª–µ–º': '—à–ª–µ–º'
    }
    
    for old, new in replacements.items():
        if old in name:
            name = name.replace(old, new)
            break
    
    return name.strip()

def load_popular_categories():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ CSV"""
    csv_file = '–†–∞—Å—á–µ—Ç—ã –•–∞–π –í—ç–π NEW - –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–æ–≤–∞—Ä–æ–≤.csv'
    
    # –ß–∏—Ç–∞–µ–º CSV —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    df_popular = pd.read_csv(csv_file, encoding='utf-8', skiprows=6)
    
    # –û—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    popular_data = []
    
    for i, row in df_popular.iterrows():
        if len(row) < 8:  # –ú–∏–Ω–∏–º—É–º —Å—Ç–æ–ª–±—Ü–æ–≤
            continue
            
        category = row.iloc[1] if pd.notna(row.iloc[1]) else ''
        material = row.iloc[2] if pd.notna(row.iloc[2]) else ''
        density = row.iloc[6] if pd.notna(row.iloc[6]) else ''
        rail_rate = row.iloc[8] if len(row) > 8 and pd.notna(row.iloc[8]) else ''
        air_rate = row.iloc[9] if len(row) > 9 and pd.notna(row.iloc[9]) else ''
        rail_density_rate = row.iloc[10] if len(row) > 10 and pd.notna(row.iloc[10]) else ''
        air_density_rate = row.iloc[11] if len(row) > 11 and pd.notna(row.iloc[11]) else ''
        certificates = row.iloc[12] if len(row) > 12 and pd.notna(row.iloc[12]) else ''
        
        if str(category).strip() != '':
            popular_data.append({
                'category_original': str(category).strip(),
                'category_normalized': normalize_category_name(category),
                'material': str(material).strip(),
                'density_recommended': density,
                'rail_rate_base': rail_rate,
                'air_rate_base': air_rate,
                'rail_rate_density': rail_density_rate,
                'air_rate_density': air_density_rate,
                'certificates': str(certificates).strip()
            })
    
    return pd.DataFrame(popular_data)

def find_missing_categories_in_db(popular_categories, db_path):
    """–ü–æ–∏—Å–∫ —Ç–æ–≤–∞—Ä–æ–≤ –≤ –ë–î –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –Ω–∞—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö"""
    conn = sqlite3.connect(db_path)
    
    missing_data = []
    
    for _, row in popular_categories.iterrows():
        category_norm = row['category_normalized']
        category_orig = row['category_original']
        
        if category_norm == '':
            continue
            
        # –ò—â–µ–º —Ç–æ–≤–∞—Ä—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        keywords = category_norm.split()
        if len(keywords) == 0:
            continue
            
        # –°—Ç—Ä–æ–∏–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞
        where_conditions = []
        for keyword in keywords:
            where_conditions.append(f"(p.title LIKE '%{keyword}%' OR p.original_title LIKE '%{keyword}%')")
        
        where_clause = ' OR '.join(where_conditions)
        
        query = f"""
        SELECT 
            p.id,
            p.original_title,
            p.title,
            pv.price_cny,
            pv.price_rub, 
            pv.price_usd,
            pv.moq,
            pv.item_weight,
            pv.transport_tariff,
            pv.cargo_density
        FROM products p
        JOIN product_variants pv ON p.id = pv.product_id
        WHERE {where_clause}
            AND pv.moq > 1
        LIMIT 50
        """
        
        try:
            df_found = pd.read_sql_query(query, conn)
            if len(df_found) > 0:
                missing_data.append({
                    'category_original': category_orig,
                    'category_normalized': category_norm,
                    'found_products': len(df_found),
                    'products_data': df_found
                })
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –¥–ª—è {category_orig}: {e}")
    
    conn.close()
    return missing_data

def create_enhanced_ranges_table():
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã —Å –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
    
    print("üéØ –°–û–ó–î–ê–ù–ò–ï –†–ê–°–®–ò–†–ï–ù–ù–û–ô –¢–ê–ë–õ–ò–¶–´ –° –ü–û–ü–£–õ–Ø–†–ù–´–ú–ò –ö–ê–¢–ï–ì–û–†–ò–Ø–ú–ò")
    print("=" * 70)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ CSV...")
    popular_df = load_popular_categories()
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(popular_df)} –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞—à–∏ –¥–∞–Ω–Ω—ã–µ
    db_path = Path(__file__).parent.parent / "promo_calculator" / "database" / "advanced_merged_products_clean.db"
    analyzer = BusinessFinalAnalyzer(db_path)
    
    print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ë–î...")
    stats, coverage, excluded_count, total_products = analyzer.run_business_analysis()
    
    # –°–æ–∑–¥–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Ç–∞–±–ª–∏—Ü—É –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤
    ranges_data = []
    
    for stat in stats:
        if not stat.get('gaussian_ranges'):
            continue
            
        row = {
            '–¢–∏–ø': stat['—Ç–∏–ø'],
            '–ö–∞—Ç–µ–≥–æ—Ä–∏—è': stat['–∫–∞—Ç–µ–≥–æ—Ä–∏—è'],
            '–†–æ–¥–∏—Ç–µ–ª—å': stat['—Ä–æ–¥–∏—Ç–µ–ª—å'],
            '–¢–æ–≤–∞—Ä–æ–≤': stat['—Ç–æ–≤–∞—Ä—ã'],
            '–ú–µ–¥–∏–∞–Ω–∞_—Ü–µ–Ω—ã_—Ä—É–±': stat.get('–º–µ–¥–∏–∞–Ω–∞_—Ü–µ–Ω—ã_rub'),
            '–ú–µ–¥–∏–∞–Ω–∞_—Ü–µ–Ω—ã_—é–∞–Ω—å': stat.get('–º–µ–¥–∏–∞–Ω–∞_—Ü–µ–Ω—ã_cny'),
            '–ú–µ–¥–∏–∞–Ω–∞_—Ç–∏—Ä–∞–∂–∞': stat.get('—Å—Ä–µ–¥–Ω–∏–π_—Ç–∏—Ä–∞–∂'),
            '–ú–µ–¥–∏–∞–Ω–∞_–ø–ª–æ—Ç–Ω–æ—Å—Ç–∏': stat.get('–º–µ–¥–∏–∞–Ω–∞_–ø–ª–æ—Ç–Ω–æ—Å—Ç–∏'),
            '–ú–µ–¥–∏–∞–Ω–∞_—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞': stat.get('–º–µ–¥–∏–∞–Ω–∞_—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞_usd'),
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω—ã –¥–ª—è –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–∏
        for field, ranges in stat['gaussian_ranges'].items():
            if field == 'price_rub':
                row.update({
                    '–¶–µ–Ω–∞_—Ä—É–±_–º–∏–Ω': ranges['lower_70'],
                    '–¶–µ–Ω–∞_—Ä—É–±_–º–∞–∫—Å': ranges['upper_70'],
                    '–¶–µ–Ω–∞_—Ä—É–±_–º–µ–¥–∏–∞–Ω–∞': ranges['median'],
                    '–¶–µ–Ω–∞_—Ä—É–±_—Å—Ä–µ–¥–Ω–µ–µ': ranges['mean'],
                    '–¶–µ–Ω–∞_—Ä—É–±_—Å—Ç–¥': ranges['std'],
                    '–¶–µ–Ω–∞_—Ä—É–±_–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ': ranges['count']
                })
            elif field == 'price_cny':
                row.update({
                    '–¶–µ–Ω–∞_—é–∞–Ω—å_–º–∏–Ω': ranges['lower_70'],
                    '–¶–µ–Ω–∞_—é–∞–Ω—å_–º–∞–∫—Å': ranges['upper_70'],
                    '–¶–µ–Ω–∞_—é–∞–Ω—å_–º–µ–¥–∏–∞–Ω–∞': ranges['median'],
                    '–¶–µ–Ω–∞_—é–∞–Ω—å_—Å—Ä–µ–¥–Ω–µ–µ': ranges['mean'],
                    '–¶–µ–Ω–∞_—é–∞–Ω—å_—Å—Ç–¥': ranges['std'],
                    '–¶–µ–Ω–∞_—é–∞–Ω—å_–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ': ranges['count']
                })
            elif field == 'avg_requested_tirage':
                row.update({
                    '–¢–∏—Ä–∞–∂_–º–∏–Ω': ranges['lower_70'],
                    '–¢–∏—Ä–∞–∂_–º–∞–∫—Å': ranges['upper_70'],
                    '–¢–∏—Ä–∞–∂_–º–µ–¥–∏–∞–Ω–∞': ranges['median'],
                    '–¢–∏—Ä–∞–∂_—Å—Ä–µ–¥–Ω–µ–µ': ranges['mean'],
                    '–¢–∏—Ä–∞–∂_—Å—Ç–¥': ranges['std'],
                    '–¢–∏—Ä–∞–∂_–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ': ranges['count']
                })
            elif field == 'cargo_density':
                row.update({
                    '–ü–ª–æ—Ç–Ω–æ—Å—Ç—å_–º–∏–Ω': ranges['lower_70'],
                    '–ü–ª–æ—Ç–Ω–æ—Å—Ç—å_–º–∞–∫—Å': ranges['upper_70'],
                    '–ü–ª–æ—Ç–Ω–æ—Å—Ç—å_–º–µ–¥–∏–∞–Ω–∞': ranges['median'],
                    '–ü–ª–æ—Ç–Ω–æ—Å—Ç—å_—Å—Ä–µ–¥–Ω–µ–µ': ranges['mean'],
                    '–ü–ª–æ—Ç–Ω–æ—Å—Ç—å_—Å—Ç–¥': ranges['std'],
                    '–ü–ª–æ—Ç–Ω–æ—Å—Ç—å_–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ': ranges['count']
                })
            elif field == 'transport_tariff':
                row.update({
                    '–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç_–º–∏–Ω': ranges['lower_70'],
                    '–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç_–º–∞–∫—Å': ranges['upper_70'],
                    '–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç_–º–µ–¥–∏–∞–Ω–∞': ranges['median'],
                    '–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç_—Å—Ä–µ–¥–Ω–µ–µ': ranges['mean'],
                    '–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç_—Å—Ç–¥': ranges['std'],
                    '–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç_–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ': ranges['count']
                })
        
        ranges_data.append(row)
    
    # –°–æ–∑–¥–∞–µ–º DataFrame
    df_ranges = pd.DataFrame(ranges_data)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    print("üîó –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏...")
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –Ω–∞—à–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    df_ranges['category_normalized'] = df_ranges['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'].apply(normalize_category_name)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
    merged_df = df_ranges.merge(
        popular_df, 
        left_on='category_normalized', 
        right_on='category_normalized', 
        how='left'
    )
    
    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º —Å—Ç–æ–ª–±—Ü—ã –¥–ª—è —è—Å–Ω–æ—Å—Ç–∏
    merged_df = merged_df.rename(columns={
        'category_original': '–ü–æ–ø—É–ª—è—Ä–Ω–∞—è_–∫–∞—Ç–µ–≥–æ—Ä–∏—è',
        'material': '–ú–∞—Ç–µ—Ä–∏–∞–ª_—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π',
        'density_recommended': '–ü–ª–æ—Ç–Ω–æ—Å—Ç—å_—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è',
        'rail_rate_base': '–°—Ç–∞–≤–∫–∞_–∂–¥_–±–∞–∑–æ–≤–∞—è',
        'air_rate_base': '–°—Ç–∞–≤–∫–∞_–∞–≤–∏–∞_–±–∞–∑–æ–≤–∞—è',
        'rail_rate_density': '–°—Ç–∞–≤–∫–∞_–∂–¥_–ø–æ_–ø–ª–æ—Ç–Ω–æ—Å—Ç–∏',
        'air_rate_density': '–°—Ç–∞–≤–∫–∞_–∞–≤–∏–∞_–ø–æ_–ø–ª–æ—Ç–Ω–æ—Å—Ç–∏',
        'certificates': '–°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã_—Ç—Ä–µ–±—É–µ–º—ã–µ'
    })
    
    # –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–π —Å—Ç–æ–ª–±–µ—Ü
    merged_df = merged_df.drop('category_normalized', axis=1)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ç–æ–≤–∞—Ä–æ–≤
    merged_df = merged_df.sort_values('–¢–æ–≤–∞—Ä–æ–≤', ascending=False)
    
    # –ò—â–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ –ë–î
    print("üîç –ü–æ–∏—Å–∫ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –ë–î...")
    missing_categories = find_missing_categories_in_db(popular_df, db_path)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    for missing in missing_categories:
        if missing['found_products'] >= 5:  # –ú–∏–Ω–∏–º—É–º 5 —Ç–æ–≤–∞—Ä–æ–≤
            print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ {missing['found_products']} —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è '{missing['category_original']}'")
            
            # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä–æ–∫—É –¥–ª—è –Ω–µ–¥–æ—Å—Ç–∞—é—â–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            products_data = missing['products_data']
            
            new_row = {
                '–¢–∏–ø': 'popular',
                '–ö–∞—Ç–µ–≥–æ—Ä–∏—è': missing['category_original'],
                '–†–æ–¥–∏—Ç–µ–ª—å': '',
                '–¢–æ–≤–∞—Ä–æ–≤': len(products_data),
                '–ú–µ–¥–∏–∞–Ω–∞_—Ü–µ–Ω—ã_—Ä—É–±': products_data['price_rub'].median() if not products_data['price_rub'].isna().all() else None,
                '–ú–µ–¥–∏–∞–Ω–∞_—Ü–µ–Ω—ã_—é–∞–Ω—å': products_data['price_cny'].median() if not products_data['price_cny'].isna().all() else None,
                '–ú–µ–¥–∏–∞–Ω–∞_—Ç–∏—Ä–∞–∂–∞': products_data['moq'].median() if not products_data['moq'].isna().all() else None,
                '–ú–µ–¥–∏–∞–Ω–∞_–ø–ª–æ—Ç–Ω–æ—Å—Ç–∏': products_data['cargo_density'].median() if not products_data['cargo_density'].isna().all() else None,
                '–ú–µ–¥–∏–∞–Ω–∞_—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞': products_data['transport_tariff'].median() if not products_data['transport_tariff'].isna().all() else None,
                '–ü–æ–ø—É–ª—è—Ä–Ω–∞—è_–∫–∞—Ç–µ–≥–æ—Ä–∏—è': missing['category_original'],
                '–ú–∞—Ç–µ—Ä–∏–∞–ª_—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π': '',
                '–ü–ª–æ—Ç–Ω–æ—Å—Ç—å_—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è': '',
                '–°—Ç–∞–≤–∫–∞_–∂–¥_–±–∞–∑–æ–≤–∞—è': '',
                '–°—Ç–∞–≤–∫–∞_–∞–≤–∏–∞_–±–∞–∑–æ–≤–∞—è': '',
                '–°—Ç–∞–≤–∫–∞_–∂–¥_–ø–æ_–ø–ª–æ—Ç–Ω–æ—Å—Ç–∏': '',
                '–°—Ç–∞–≤–∫–∞_–∞–≤–∏–∞_–ø–æ_–ø–ª–æ—Ç–Ω–æ—Å—Ç–∏': '',
                '–°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã_—Ç—Ä–µ–±—É–µ–º—ã–µ': ''
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ç–∞–±–ª–∏—Ü—É
            merged_df = pd.concat([merged_df, pd.DataFrame([new_row])], ignore_index=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    output_file = Path(__file__).parent / "ENHANCED_CATEGORIES_RANGES.xlsx"
    
    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
        # –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
        merged_df.to_excel(writer, sheet_name='–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ_–¥–∏–∞–ø–∞–∑–æ–Ω—ã', index=False)
        
        # –¢–û–ü-20
        df_top20 = merged_df.head(20)
        df_top20.to_excel(writer, sheet_name='–¢–û–ü_20_—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ', index=False)
        
        # –¢–æ–ª—å–∫–æ —Å –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        df_with_popular = merged_df[merged_df['–ü–æ–ø—É–ª—è—Ä–Ω–∞—è_–∫–∞—Ç–µ–≥–æ—Ä–∏—è'].notna()]
        df_with_popular.to_excel(writer, sheet_name='–°_–ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏_–¥–∞–Ω–Ω—ã–º–∏', index=False)
        
        # –¢–æ–ª—å–∫–æ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        df_popular_only = merged_df[merged_df['–¢–∏–ø'] == 'popular']
        df_popular_only.to_excel(writer, sheet_name='–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ_–∫–∞—Ç–µ–≥–æ—Ä–∏–∏', index=False)
    
    print(f"‚úÖ –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å–æ–∑–¥–∞–Ω–∞: {output_file}")
    print(f"üìä –õ–∏—Å—Ç—ã:")
    print(f"  1. '–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ_–¥–∏–∞–ø–∞–∑–æ–Ω—ã' - {len(merged_df)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å –ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
    print(f"  2. '–¢–û–ü_20_—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ' - –ª–∏–¥–µ—Ä—ã —Å –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
    print(f"  3. '–°_–ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏_–¥–∞–Ω–Ω—ã–º–∏' - –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ CSV")
    print(f"  4. '–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ_–∫–∞—Ç–µ–≥–æ—Ä–∏–∏' - —Ç–æ–ª—å–∫–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≤ –ë–î –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏")
    print()
    print(f"üéØ –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –°–¢–û–õ–ë–¶–´:")
    print(f"  ‚Ä¢ –ú–∞—Ç–µ—Ä–∏–∞–ª_—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã")
    print(f"  ‚Ä¢ –ü–ª–æ—Ç–Ω–æ—Å—Ç—å_—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å")
    print(f"  ‚Ä¢ –°—Ç–∞–≤–∫–∞_–∂–¥_–±–∞–∑–æ–≤–∞—è - –±–∞–∑–æ–≤–∞—è —Å—Ç–∞–≤–∫–∞ –ñ–î")
    print(f"  ‚Ä¢ –°—Ç–∞–≤–∫–∞_–∞–≤–∏–∞_–±–∞–∑–æ–≤–∞—è - –±–∞–∑–æ–≤–∞—è —Å—Ç–∞–≤–∫–∞ –∞–≤–∏–∞")
    print(f"  ‚Ä¢ –°—Ç–∞–≤–∫–∞_–∂–¥_–ø–æ_–ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ - —Å—Ç–∞–≤–∫–∞ –ñ–î –ø–æ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏")
    print(f"  ‚Ä¢ –°—Ç–∞–≤–∫–∞_–∞–≤–∏–∞_–ø–æ_–ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ - —Å—Ç–∞–≤–∫–∞ –∞–≤–∏–∞ –ø–æ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏")
    print(f"  ‚Ä¢ –°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã_—Ç—Ä–µ–±—É–µ–º—ã–µ - —Ç—Ä–µ–±—É–µ–º—ã–µ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã")
    
    analyzer.conn.close()
    return output_file

if __name__ == "__main__":
    create_enhanced_ranges_table()
