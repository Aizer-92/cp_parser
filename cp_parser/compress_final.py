#!/usr/bin/env python3
"""
–§–ò–ù–ê–õ–¨–ù–û–ï –°–ñ–ê–¢–ò–ï - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–æ—Ç–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ FTP —Ñ–∞–π–ª–æ–≤
"""

from ftplib import FTP_TLS
from PIL import Image
import io
import re
import json
import os
import pandas as pd

# FTP –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
FTP_HOST = "ftp.ru1.storage.beget.cloud"
FTP_USER = "RECD00AQJIM4300MLJ0W"
FTP_PASS = "FIucJ3i9iIWZ5ieJvabvI0OxEn2Yv4gG5XRUeSNf"
FTP_DIR = "/73d16f7545b3-promogoods/images"

BATCH_SIZE = 50
PROGRESS_FILE = 'compression_progress.json'

print("=" * 80)
print("üóúÔ∏è  –§–ò–ù–ê–õ–¨–ù–û–ï –°–ñ–ê–¢–ò–ï –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô")
print("=" * 80)

# –ß–∏—Ç–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ CSV (–±—ã—Å—Ç—Ä–æ!)
print("\nüìÑ –ß–∏—Ç–∞—é —Å–ø–∏—Å–æ–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ FTP_FILES_ANALYSIS.csv...")
df = pd.read_csv('FTP_FILES_ANALYSIS.csv')
# –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤ –∏–∑ URLs
ftp_filenames = set()
for url in df['image_url']:
    filename = url.split('/')[-1]
    ftp_filenames.add(filename)
print(f"‚úÖ –§–∞–π–ª–æ–≤ –Ω–∞ FTP: {len(ftp_filenames):,}")

# –ß–∏—Ç–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ >1MB –∏–∑ –∞–Ω–∞–ª–∏–∑–∞
print("\nüìÑ –ß–∏—Ç–∞—é —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ >1MB –∏–∑ –∞–Ω–∞–ª–∏–∑–∞...")
all_files = []
with open('STORAGE_ANALYSIS_20251012_1455.txt', 'r', encoding='utf-8') as f:
    for line in f:
        match = re.search(r'(\d+\.\d+)\s+MB\s+-\s+(.+\.png)', line)
        if match:
            size_mb = float(match.group(1))
            filename = match.group(2).strip()
            if size_mb >= 1.0:
                all_files.append({
                    'filename': filename,
                    'size_mb': size_mb
                })

print(f"‚úÖ –§–∞–π–ª–æ–≤ >1MB –≤ –∞–Ω–∞–ª–∏–∑–µ: {len(all_files):,}")

# –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ
existing_files = [f for f in all_files if f['filename'] in ftp_filenames]
print(f"‚úÖ –°—É—â–µ—Å—Ç–≤—É–µ—Ç –Ω–∞ FTP: {len(existing_files):,}")

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –µ—Å–ª–∏ –µ—Å—Ç—å
processed_files = set()
if os.path.exists(PROGRESS_FILE):
    with open(PROGRESS_FILE, 'r') as f:
        data = json.load(f)
        processed_files = set(data.get('processed', []))
    print(f"üìä –£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(processed_files):,}")

# –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
remaining_files = [f for f in existing_files if f['filename'] not in processed_files]
print(f"üìä –û—Å—Ç–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å: {len(remaining_files):,}")
total_size = sum(f['size_mb'] for f in remaining_files)
print(f"üìä –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size:.2f} MB ({total_size/1024:.2f} GB)")

if len(remaining_files) == 0:
    print("\n‚úÖ –í–°–ï –§–ê–ô–õ–´ –£–ñ–ï –û–ë–†–ê–ë–û–¢–ê–ù–´!")
    exit(0)

# –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ FTP —Å retry
print("\nüîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ FTP...")
import time
for attempt in range(5):
    try:
        ftp = FTP_TLS(FTP_HOST)
        ftp.login(FTP_USER, FTP_PASS)
        ftp.prot_p()
        ftp.cwd(FTP_DIR)
        print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ")
        break
    except Exception as e:
        print(f"‚ö†Ô∏è  –ü–æ–ø—ã—Ç–∫–∞ {attempt+1}/5 –Ω–µ —É–¥–∞–ª–∞—Å—å: {str(e)[:50]}")
        if attempt < 4:
            time.sleep(5)
        else:
            raise

# –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏
print(f"\nüóúÔ∏è  –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –±–∞—Ç—á–∞–º–∏ –ø–æ {BATCH_SIZE} —Ñ–∞–π–ª–æ–≤...")
print("   (–ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞)\n")

total_success = 0
total_errors = 0
total_original = 0
total_compressed = 0
resized_count = 0

batch_num = 1
for i in range(0, len(remaining_files), BATCH_SIZE):
    batch = remaining_files[i:i+BATCH_SIZE]
    print(f"\n{'='*80}")
    print(f"üì¶ –ë–ê–¢–ß #{batch_num} ({i+1}-{min(i+BATCH_SIZE, len(remaining_files))} –∏–∑ {len(remaining_files):,})")
    print(f"{'='*80}\n")
    
    for j, file_info in enumerate(batch, 1):
        filename = file_info['filename']
        current_num = i + j
        
        try:
            # –°–∫–∞—á–∏–≤–∞–µ–º PNG
            bio = io.BytesIO()
            ftp.retrbinary(f'RETR {filename}', bio.write)
            bio.seek(0)
            
            if len(bio.getvalue()) == 0:
                raise Exception("–§–∞–π–ª –ø—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ —Å–∫–∞—á–∞–ª—Å—è")
            
            # –û—Ç–∫—Ä—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            img = Image.open(bio)
            original_size = len(bio.getvalue())
            total_original += original_size
            
            # –†–µ—Å–∞–π–∑ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            width, height = img.size
            was_resized = False
            if max(width, height) > 1920:
                if width > height:
                    img = img.resize((1920, int(height * (1920 / width))), Image.Resampling.LANCZOS)
                else:
                    img = img.resize((int(width * (1920 / height)), 1920), Image.Resampling.LANCZOS)
                was_resized = True
                resized_count += 1
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ WebP
            webp_bio = io.BytesIO()
            img.save(webp_bio, 'WEBP', quality=85)
            compressed_size = len(webp_bio.getvalue())
            total_compressed += compressed_size
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º WebP
            webp_filename = filename.replace('.png', '.webp')
            webp_bio.seek(0)
            ftp.storbinary(f'STOR {webp_filename}', webp_bio)
            
            total_success += 1
            processed_files.add(filename)
            
            savings = (1 - compressed_size/original_size)*100 if original_size > 0 else 0
            resize_mark = "üìê" if was_resized else ""
            print(f"   [{current_num}/{len(remaining_files)}] ‚úÖ {resize_mark} {filename[:40]}... ({savings:.0f}% ‚Üì)")
        
        except Exception as e:
            total_errors += 1
            processed_files.add(filename)
            print(f"   [{current_num}/{len(remaining_files)}] ‚ùå {filename[:40]}... - {str(e)[:50]}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞
    with open(PROGRESS_FILE, 'w') as f:
        json.dump({
            'processed': list(processed_files),
            'total_success': total_success,
            'total_errors': total_errors,
            'total_original_mb': total_original / (1024*1024),
            'total_compressed_mb': total_compressed / (1024*1024),
            'resized_count': resized_count
        }, f)
    
    batch_progress = (current_num / len(remaining_files)) * 100
    print(f"\nüíæ –ü—Ä–æ–≥—Ä–µ—Å—Å: {total_success} —É—Å–ø–µ—à–Ω–æ, {total_errors} –æ—à–∏–±–æ–∫ ({batch_progress:.1f}% –∑–∞–≤–µ—Ä—à–µ–Ω–æ)")
    batch_num += 1

ftp.quit()

# –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
savings_mb = (total_original - total_compressed) / (1024*1024)
savings_gb = savings_mb / 1024
compression_ratio = (1 - total_compressed/total_original)*100 if total_original > 0 else 0

print("\n" + "="*80)
print("üìä –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
print("="*80)
print(f"\n‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_success:,}")
print(f"   –ò–∑ –Ω–∏—Ö —Å —Ä–µ—Å–∞–π–∑–æ–º: {resized_count:,}")
print(f"‚ùå –û—à–∏–±–æ–∫: {total_errors:,}")
print(f"\nüíæ –≠–∫–æ–Ω–æ–º–∏—è:")
print(f"   –ë—ã–ª–æ: {total_original/(1024*1024):.2f} MB ({total_original/(1024*1024*1024):.2f} GB)")
print(f"   –°—Ç–∞–ª–æ: {total_compressed/(1024*1024):.2f} MB ({total_compressed/(1024*1024*1024):.2f} GB)")
print(f"   –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {savings_mb:.2f} MB ({savings_gb:.2f} GB)")
print(f"   –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è: {compression_ratio:.1f}%")
if total_success > 0:
    print(f"\nüìä –°—Ä–µ–¥–Ω—è—è —ç–∫–æ–Ω–æ–º–∏—è –Ω–∞ —Ñ–∞–π–ª: {savings_mb/total_success:.2f} MB")
print("="*80)

