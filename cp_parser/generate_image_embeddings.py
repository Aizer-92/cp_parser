#!/usr/bin/env python3
"""
–ì–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Ç–æ–≤–∞—Ä–æ–≤.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç CLIP –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –≥–ª–∞–≤–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (is_main_image = TRUE).
"""

import os
import sys
import time
from sqlalchemy import create_engine, text
from dotenv import load_dotenv
from PIL import Image
import requests
from io import BytesIO

load_dotenv()

# –¶–≤–µ—Ç–∞
GREEN = '\033[92m'
YELLOW = '\033[93m'
RED = '\033[91m'
BLUE = '\033[94m'
RESET = '\033[0m'

def print_success(text):
    print(f"{GREEN}‚úÖ {text}{RESET}")

def print_error(text):
    print(f"{RED}‚ùå {text}{RESET}")

def print_info(text):
    print(f"{BLUE}‚ÑπÔ∏è  {text}{RESET}")

def print_warning(text):
    print(f"{YELLOW}‚ö†Ô∏è  {text}{RESET}")

def load_clip_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç CLIP –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embeddings"""
    print_info("–ó–∞–≥—Ä—É–∑–∫–∞ CLIP –º–æ–¥–µ–ª–∏...")
    
    try:
        # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å sentence-transformers (–ø—Ä–æ—â–µ –∏ –±—ã—Å—Ç—Ä–µ–µ)
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer('clip-ViT-B-32')
        print_success("CLIP –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (sentence-transformers)")
        return model, "sentence-transformers"
    except ImportError:
        print_warning("sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        print_info("–£—Å—Ç–∞–Ω–æ–≤–∫–∞: pip install sentence-transformers")
        print_info("–ò–ª–∏: pip install transformers pillow torch")
        
        try:
            # Fallback –Ω–∞ transformers
            from transformers import CLIPProcessor, CLIPModel
            import torch
            
            model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
            processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
            print_success("CLIP –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (transformers)")
            return (model, processor), "transformers"
        except ImportError:
            print_error("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫ –¥–ª—è CLIP!")
            print_info("–£—Å—Ç–∞–Ω–æ–≤–∏ –æ–¥–Ω—É –∏–∑:")
            print("  pip install sentence-transformers")
            print("  pip install transformers pillow torch")
            sys.exit(1)

def generate_embedding(image_source, model, model_type, is_local=False):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç embedding –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–∏–∑ URL –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞)"""
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        if is_local:
            # –õ–æ–∫–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª
            image = Image.open(image_source).convert('RGB')
        else:
            # URL
            response = requests.get(image_source, timeout=10)
            image = Image.open(BytesIO(response.content)).convert('RGB')
        
        if model_type == "sentence-transformers":
            # sentence-transformers
            embedding = model.encode(image)
            return embedding.tolist()
        
        elif model_type == "transformers":
            # transformers (–±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π)
            import torch
            clip_model, processor = model
            
            inputs = processor(images=image, return_tensors="pt")
            with torch.no_grad():
                image_features = clip_model.get_image_features(**inputs)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            embedding = image_features / image_features.norm(dim=-1, keepdim=True)
            return embedding[0].tolist()
        
    except Exception as e:
        return None

def main():
    print(f"\n{BLUE}{'=' * 80}{RESET}")
    print(f"{BLUE}{'–ì–ï–ù–ï–†–ê–¶–ò–Ø IMAGE EMBEDDINGS'.center(80)}{RESET}")
    print(f"{BLUE}{'=' * 80}{RESET}\n")
    
    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
    main_db_url = os.getenv('DATABASE_URL_PRIVATE') or os.getenv('DATABASE_URL')
    vector_db_url = os.getenv('VECTOR_DATABASE_URL')
    
    if not main_db_url or not vector_db_url:
        print_error("–ù–µ –Ω–∞–π–¥–µ–Ω—ã DATABASE_URL –∏/–∏–ª–∏ VECTOR_DATABASE_URL")
        sys.exit(1)
    
    print_info(f"–û—Å–Ω–æ–≤–Ω–∞—è –ë–î: {main_db_url[:50]}...")
    print_info(f"–í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î: {vector_db_url[:50]}...")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    model, model_type = load_clip_model()
    
    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –æ—Å–Ω–æ–≤–Ω–æ–π –ë–î
    print_info("\n1Ô∏è‚É£  –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –æ—Å–Ω–æ–≤–Ω–æ–π –ë–î...")
    main_engine = create_engine(main_db_url, pool_pre_ping=True)
    
    with main_engine.connect() as conn:
        # –ü–æ–ª—É—á–∞–µ–º –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–≤–∞—Ä–∞ (—Å URL –∏–ª–∏ local_path)
        result = conn.execute(text("""
            SELECT COUNT(DISTINCT product_id)
            FROM product_images 
            WHERE product_id IS NOT NULL
            AND (
                (image_url IS NOT NULL AND image_url != '')
                OR (image_filename IS NOT NULL AND image_filename != '')
            )
        """))
        total_images = result.scalar()
    
    print_success(f"–ù–∞–π–¥–µ–Ω–æ {total_images} –≥–ª–∞–≤–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ pgvector –ë–î
    print_info("\n2Ô∏è‚É£  –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ pgvector –ë–î...")
    vector_engine = create_engine(vector_db_url, pool_pre_ping=True)
    
    with vector_engine.connect() as conn:
        result = conn.execute(text("SELECT COUNT(*) FROM image_embeddings"))
        existing_count = result.scalar()
    
    print_info(f"–í pgvector –ë–î —É–∂–µ –µ—Å—Ç—å {existing_count} embeddings")
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings
    print_info("\n3Ô∏è‚É£  –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings...")
    print_warning("   –≠—Ç–æ –∑–∞–π–º–µ—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ 2-3 —Å–µ–∫—É–Ω–¥—ã –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ")
    print_warning(f"   –û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è: ~{total_images * 2.5 / 60:.0f} –º–∏–Ω—É—Ç")
    print_success(f"   –ù–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–ª—è {total_images} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π!\n")
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏
    batch_size = 100
    offset = 0
    success_count = 0
    error_count = 0
    
    start_time = time.time()
    
    while offset < total_images:
        # –ß–∏—Ç–∞–µ–º batch –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–π –ë–î (–æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ —Ç–æ–≤–∞—Ä, –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Å—Ç–æ–ª–±—Ü—É –ê)
        with main_engine.connect() as conn:
            images = conn.execute(text("""
                SELECT DISTINCT ON (pi.product_id) 
                    pi.id, 
                    pi.product_id, 
                    pi.image_url,
                    pi.image_filename
                FROM product_images pi
                WHERE pi.product_id IS NOT NULL
                AND (
                    (pi.image_url IS NOT NULL AND pi.image_url != '')
                    OR (pi.image_filename IS NOT NULL AND pi.image_filename != '')
                )
                ORDER BY pi.product_id, 
                         CASE WHEN pi.column_number = 1 THEN 0 ELSE 1 END,
                         pi.id
                LIMIT :limit OFFSET :offset
            """), {"limit": batch_size, "offset": offset}).fetchall()
        
        if not images:
            break
        
        print(f"\n   üì¶ Batch {offset // batch_size + 1}: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embeddings
        for i, (image_id, product_id, image_url, image_filename) in enumerate(images, 1):
            try:
                # –§–æ—Ä–º–∏—Ä—É–µ–º URL (–µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä—è–º–æ–π URL - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ, –∏–Ω–∞—á–µ —Ñ–æ—Ä–º–∏—Ä—É–µ–º –∏–∑ filename)
                if image_url and image_url.strip():
                    final_url = image_url
                elif image_filename and image_filename.strip():
                    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–±–ª–∞—á–Ω—ã–π URL –∏–∑ filename
                    final_url = f"https://s3.ru1.storage.beget.cloud/73d16f7545b3-promogoods/images/{image_filename}"
                else:
                    error_count += 1
                    continue
                
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embedding
                embedding = generate_embedding(final_url, model, model_type, is_local=False)
                
                if embedding is None:
                    error_count += 1
                    continue
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ pgvector –ë–î
                with vector_engine.begin() as conn:
                    conn.execute(text("""
                        INSERT INTO image_embeddings 
                            (image_id, product_id, image_embedding, image_url, model_version)
                        VALUES 
                            (:image_id, :product_id, :embedding, :url, :model)
                        ON CONFLICT (image_id) DO UPDATE
                        SET image_embedding = EXCLUDED.image_embedding,
                            updated_at = NOW()
                    """), {
                        "image_id": image_id,
                        "product_id": product_id,
                        "embedding": embedding,
                        "url": final_url[:200],  # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ URL
                        "model": f"{model_type}/clip-vit-base-patch32"
                    })
                
                success_count += 1
                
                # –ü—Ä–æ–≥—Ä–µ—Å—Å
                if success_count % 10 == 0:
                    elapsed = time.time() - start_time
                    rate = success_count / elapsed if elapsed > 0 else 0
                    remaining = (total_images - success_count) / rate if rate > 0 else 0
                    print(f"      –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {success_count}/{total_images} | "
                          f"–°–∫–æ—Ä–æ—Å—Ç—å: {rate:.1f}/—Å–µ–∫ | ETA: {remaining/60:.0f}–º–∏–Ω")
                
            except Exception as e:
                print_error(f"   –û—à–∏–±–∫–∞ –¥–ª—è image_id={image_id}: {e}")
                error_count += 1
                continue
        
        offset += batch_size
    
    elapsed_time = time.time() - start_time
    
    print(f"\n{GREEN}{'=' * 80}{RESET}")
    print_success(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print_success(f"   –£—Å–ø–µ—à–Ω–æ: {success_count}")
    print(f"   {RED}–û—à–∏–±–æ–∫: {error_count}{RESET}")
    print_success(f"   –í—Ä–µ–º—è: {elapsed_time / 60:.1f} –º–∏–Ω—É—Ç")
    print_success(f"   –°–∫–æ—Ä–æ—Å—Ç—å: {success_count / elapsed_time:.1f} embeddings/—Å–µ–∫")
    print(f"{GREEN}{'=' * 80}{RESET}\n")
    
    print_info("üìù –ß—Ç–æ –¥–∞–ª—å—à–µ:")
    print("1. –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–π –ø–æ–∏—Å–∫ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –≤ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å")
    print("2. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π –∑–∞–≥—Ä—É–∑–∫—É —Ñ–æ—Ç–æ –∏ –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Ç–æ–≤–∞—Ä–æ–≤")
    print("3. –û—Ü–µ–Ω–∏ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

if __name__ == "__main__":
    main()

