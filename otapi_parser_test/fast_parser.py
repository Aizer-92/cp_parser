#!/usr/bin/env python3
"""
–ë—ã—Å—Ç—Ä—ã–π –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
"""
import json
import time
import pandas as pd
from pathlib import Path
from typing import Dict, List, Optional, Any, Set
from datetime import datetime
import logging
import os
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue
import signal
import sys
import requests

from enhanced_parser import EnhancedOTAPIParser
from config import OTAPIConfig

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–æ–ª—å–∫–æ –≤ —Ñ–∞–π–ª
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('fast_parser.log', encoding='utf-8')
    ]
)

class FastParser:
    """–ë—ã—Å—Ç—Ä—ã–π –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏"""
    
    def __init__(self, max_workers: int = 12):
        self.config = OTAPIConfig()
        self.max_workers = max_workers
        
        # –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Ç–æ–∫–∞
        self.parsers = {}
        
        # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
        self.main_project_path = Path("../promo_calculator")
        self.existing_data_path = self.main_project_path / "database" / "full-clear-database-09_07_2025.xlsx"
        self.offer_ids_path = self.main_project_path / "parsing_project" / "data" / "unique_offer_ids.json"
        
        # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.data_dir = Path("data")
        self.results_dir = self.data_dir / "results"
        self.chunk_dir = self.results_dir / "chunks"
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.data_dir.mkdir(exist_ok=True)
        self.results_dir.mkdir(exist_ok=True)
        self.chunk_dir.mkdir(exist_ok=True)
        
        # –§–∞–π–ª—ã –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.progress_file = self.results_dir / "fast_parsing_progress.json"
        self.stats_file = self.results_dir / "fast_parsing_stats.json"
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        self.existing_data = {}
        self.offer_ids = []
        self.load_existing_data()
        self.load_offer_ids()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        self.progress_data = self.load_progress()
        self.processed_ids = set(self.progress_data.get('processed_ids', []))
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = self.progress_data.get('stats', {
            'successful': 0,
            'failed': 0,
            'errors': []
        })
        
        # –¢–µ–∫—É—â–∏–π —á–∞–Ω–∫
        self.current_chunk = []
        self.chunk_size = 100  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
        self.chunk_counter = len(list(self.chunk_dir.glob("fast_chunk_*.json"))) + 1
        
        # –ü–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã
        self.lock = threading.Lock()
        self.running = True
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è graceful shutdown
        signal.signal(signal.SIGINT, self.signal_handler)
        signal.signal(signal.SIGTERM, self.signal_handler)
        
        print(f"üöÄ –ë—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        print(f"   - –ü–æ—Ç–æ–∫–æ–≤: {max_workers}")
        print(f"   - –£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(self.processed_ids)} —Ç–æ–≤–∞—Ä–æ–≤")
        print("=" * 60)
    
    def signal_handler(self, signum, frame):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è graceful shutdown"""
        print("\nüõë –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è, –∑–∞–≤–µ—Ä—à–∞–µ–º —Ä–∞–±–æ—Ç—É...")
        self.running = False
        self.save_progress()
        sys.exit(0)
    
    def get_parser_for_thread(self, thread_id: int) -> EnhancedOTAPIParser:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–∞—Ä—Å–µ—Ä –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞"""
        if thread_id not in self.parsers:
            self.parsers[thread_id] = EnhancedOTAPIParser()
        return self.parsers[thread_id]
    
    def load_existing_data(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–π –±–∞–∑—ã"""
        try:
            if self.existing_data_path.exists():
                df = pd.read_excel(self.existing_data_path)
                for _, row in df.iterrows():
                    item_id = str(row.get('item_id', ''))
                    if item_id:
                        self.existing_data[item_id] = {
                            'title': row.get('title', ''),
                            'price': row.get('price', ''),
                            'vendor': row.get('vendor', ''),
                            'brand': row.get('brand', ''),
                            'url': row.get('url', '')
                        }
                logging.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.existing_data)} —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–π –±–∞–∑—ã")
            else:
                logging.warning(f"–§–∞–π–ª –æ—Å–Ω–æ–≤–Ω–æ–π –±–∞–∑—ã –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.existing_data_path}")
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –±–∞–∑—ã: {e}")
    
    def load_offer_ids(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç ID —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        try:
            if self.offer_ids_path.exists():
                with open(self.offer_ids_path, 'r', encoding='utf-8') as f:
                    self.offer_ids = json.load(f)
                logging.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.offer_ids)} ID –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")
            else:
                logging.error(f"–§–∞–π–ª —Å ID –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.offer_ids_path}")
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ ID: {e}")
    
    def load_progress(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        if self.progress_file.exists():
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                logging.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω –ø—Ä–æ–≥—Ä–µ—Å—Å: {len(data.get('processed_ids', []))} —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤")
                return data
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: {e}")
        
        return {
            'processed_ids': [],
            'stats': {
                'successful': 0,
                'failed': 0,
                'errors': []
            },
            'last_updated': datetime.now().isoformat()
        }
    
    def save_progress(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        try:
            with self.lock:
                progress_data = {
                    'processed_ids': list(self.processed_ids),
                    'stats': self.stats,
                    'last_updated': datetime.now().isoformat()
                }
                
                with open(self.progress_file, 'w', encoding='utf-8') as f:
                    json.dump(progress_data, f, ensure_ascii=False, indent=2)
                
                logging.info(f"–ü—Ä–æ–≥—Ä–µ—Å—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {len(self.processed_ids)} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤")
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: {e}")
    
    def save_chunk(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫"""
        try:
            if not self.current_chunk:
                return
            
            chunk_filename = f"fast_chunk_{self.chunk_counter:03d}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            chunk_path = self.chunk_dir / chunk_filename
            
            with open(chunk_path, 'w', encoding='utf-8') as f:
                json.dump(self.current_chunk, f, ensure_ascii=False, indent=2)
            
            logging.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω —á–∞–Ω–∫ {self.chunk_counter}: {len(self.current_chunk)} —Ç–æ–≤–∞—Ä–æ–≤")
            
            self.current_chunk = []
            self.chunk_counter += 1
            
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–∞–Ω–∫–∞: {e}")
    
    def parse_single_product(self, item_id: str, thread_id: int) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç –æ–¥–∏–Ω —Ç–æ–≤–∞—Ä —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏"""
        parser = self.get_parser_for_thread(thread_id)
        
        try:
            # –ë—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –±–µ–∑ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
            result = parser.parse_product(item_id)
            
            if result:
                with self.lock:
                    self.stats['successful'] += 1
                title = result.get('title', '')[:30] if result.get('title') else 'Unknown'
                print(f"‚úì {item_id}: {title}...")
                return result
            else:
                with self.lock:
                    self.stats['failed'] += 1
                    self.stats['errors'].append({
                        'item_id': item_id,
                        'error': 'No data received',
                        'timestamp': datetime.now().isoformat()
                    })
                print(f"‚úó {item_id}: No data received")
                return None
                
        except Exception as e:
            with self.lock:
                self.stats['failed'] += 1
                self.stats['errors'].append({
                    'item_id': item_id,
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                })
            print(f"‚ùå {item_id}: {str(e)[:50]}")
            return None
    
    def parse_products_batch(self, limit: int = 1000, start_from: int = 0) -> bool:
        """–ü–∞—Ä—Å–∏—Ç –±–∞—Ç—á —Ç–æ–≤–∞—Ä–æ–≤ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏"""
        try:
            print(f"üéØ –ó–∞–ø—É—Å–∫–∞–µ–º –±—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–∏–Ω–≥ {limit} —Ç–æ–≤–∞—Ä–æ–≤ —Å {self.max_workers} –ø–æ—Ç–æ–∫–∞–º–∏...")
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ ID –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
            remaining_ids = [id for id in self.offer_ids if id not in self.processed_ids]
            ids_to_parse = remaining_ids[start_from:start_from + limit]
            
            if not ids_to_parse:
                print("‚ÑπÔ∏è –ù–µ—Ç —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")
                return True
            
            print(f"üìã –ë—É–¥–µ–º –ø–∞—Ä—Å–∏—Ç—å {len(ids_to_parse)} —Ç–æ–≤–∞—Ä–æ–≤")
            print("=" * 60)
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏
                future_to_id = {
                    executor.submit(self.parse_single_product, item_id, i % self.max_workers): item_id
                    for i, item_id in enumerate(ids_to_parse)
                }
                
                completed = 0
                start_time = time.time()
                
                for future in as_completed(future_to_id):
                    if not self.running:
                        print("\nüõë –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏, –∑–∞–≤–µ—Ä—à–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥...")
                        break
                    
                    item_id = future_to_id[future]
                    
                    try:
                        result = future.result()
                        
                        if result:
                            self.current_chunk.append(result)
                            
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫ –µ—Å–ª–∏ –æ–Ω –∑–∞–ø–æ–ª–Ω–µ–Ω
                            if len(self.current_chunk) >= self.chunk_size:
                                self.save_chunk()
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º ID –≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
                        with self.lock:
                            self.processed_ids.add(item_id)
                        
                        completed += 1
                        
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 50 —Ç–æ–≤–∞—Ä–æ–≤
                        if completed % 50 == 0:
                            elapsed = time.time() - start_time
                            rate = completed / elapsed if elapsed > 0 else 0
                            remaining = len(ids_to_parse) - completed
                            eta = remaining / rate if rate > 0 else 0
                            
                            print(f"\nüìà –ü–†–û–ì–†–ï–°–°: {completed}/{len(ids_to_parse)} ({(completed/len(ids_to_parse)*100):.1f}%)")
                            print(f"   –°–∫–æ—Ä–æ—Å—Ç—å: {rate:.1f} —Ç–æ–≤–∞—Ä–æ–≤/—Å–µ–∫")
                            print(f"   –û—Å—Ç–∞–ª–æ—Å—å –≤—Ä–µ–º–µ–Ω–∏: {eta/60:.1f} –º–∏–Ω—É—Ç")
                            print(f"   –£—Å–ø–µ—à–Ω–æ: {self.stats['successful']}, –û—à–∏–±–æ–∫: {self.stats['failed']}")
                            
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                            self.save_progress()
                        
                    except Exception as e:
                        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {item_id}: {e}")
                        with self.lock:
                            self.stats['failed'] += 1
                            self.stats['errors'].append({
                                'item_id': item_id,
                                'error': str(e),
                                'timestamp': datetime.now().isoformat()
                            })
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
            if self.current_chunk:
                self.save_chunk()
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_time = time.time() - start_time
            total_rate = completed / total_time if total_time > 0 else 0
            
            print(f"\nüéâ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!")
            print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {completed}/{len(ids_to_parse)} —Ç–æ–≤–∞—Ä–æ–≤")
            print(f"   –í—Ä–µ–º—è: {total_time/60:.1f} –º–∏–Ω—É—Ç")
            print(f"   –°–∫–æ—Ä–æ—Å—Ç—å: {total_rate:.1f} —Ç–æ–≤–∞—Ä–æ–≤/—Å–µ–∫")
            print(f"   –£—Å–ø–µ—à–Ω–æ: {self.stats['successful']} —Ç–æ–≤–∞—Ä–æ–≤")
            print(f"   –û—à–∏–±–æ–∫: {self.stats['failed']} —Ç–æ–≤–∞—Ä–æ–≤")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å
            self.save_progress()
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            logging.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            return False
    
    def merge_chunks_to_sqlite(self):
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ —á–∞–Ω–∫–∏ –≤ SQLite –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        try:
            print("üîÑ –û–±—ä–µ–¥–∏–Ω—è–µ–º —á–∞–Ω–∫–∏ –≤ SQLite –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...")
            
            # –°–æ–∑–¥–∞–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            db_path = self.results_dir / "fast_products_database.db"
            
            import sqlite3
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS products (
                    item_id TEXT PRIMARY KEY,
                    title TEXT,
                    price TEXT,
                    vendor TEXT,
                    brand TEXT,
                    url TEXT,
                    description TEXT,
                    specifications TEXT,
                    images TEXT
                )
            ''')
            
            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —á–∞–Ω–∫–∏
            chunk_files = list(self.chunk_dir.glob("fast_chunk_*.json"))
            total_products = 0
            
            for chunk_file in chunk_files:
                print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {chunk_file.name}...")
                
                with open(chunk_file, 'r', encoding='utf-8') as f:
                    chunk_data = json.load(f)
                
                for product in chunk_data:
                    try:
                        cursor.execute('''
                            INSERT OR REPLACE INTO products 
                            (item_id, title, price, vendor, brand, url, description, specifications, images)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                        ''', (
                            product.get('item_id'),
                            product.get('title'),
                            product.get('price'),
                            json.dumps(product.get('vendor', {})) if isinstance(product.get('vendor'), dict) else str(product.get('vendor', '')),
                            json.dumps(product.get('brand', {})) if isinstance(product.get('brand'), dict) else str(product.get('brand', '')),
                            product.get('url'),
                            json.dumps(product.get('description', '')),
                            json.dumps(product.get('specifications', {})),
                            json.dumps(product.get('images', []))
                        ))
                        total_products += 1
                    except Exception as e:
                        print(f"–û—à–∏–±–∫–∞ –≤—Å—Ç–∞–≤–∫–∏ —Ç–æ–≤–∞—Ä–∞ {product.get('item_id')}: {e}")
            
            conn.commit()
            conn.close()
            
            print(f"‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ–∑–¥–∞–Ω–∞: {db_path}")
            print(f"   –¢–æ–≤–∞—Ä–æ–≤ –≤ –±–∞–∑–µ: {total_products}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")
            return False
