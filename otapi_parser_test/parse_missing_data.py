#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –º–∞—Å—Å–æ–≤–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–æ–≤–∞—Ä–æ–≤ –±–µ–∑ –æ–ø–∏—Å–∞–Ω–∏–π –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
"""
import sqlite3
import json
import time
from pathlib import Path
from datetime import datetime
from otapi_parser import OTAPIParser

def parse_missing_data():
    """–ü–∞—Ä—Å–∏—Ç —Ç–æ–≤–∞—Ä—ã –±–µ–∑ –æ–ø–∏—Å–∞–Ω–∏–π –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫"""
    print("üöÄ –ú–ê–°–°–û–í–´–ô –ü–ê–†–°–ò–ù–ì –¢–û–í–ê–†–û–í –ë–ï–ó –û–ü–ò–°–ê–ù–ò–ô –ò –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö")
    print("=" * 60)
    
    # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –ë–î
    db_path = Path("data/results/all_chunks_database.db")
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–≤–∞—Ä—ã –±–µ–∑ –æ–ø–∏—Å–∞–Ω–∏–π –ò–õ–ò –±–µ–∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
    cursor.execute('''
        SELECT DISTINCT p.item_id, p.title, p.chunk_source, p.chunk_type
        FROM products p
        WHERE (p.description IS NULL OR p.description = '' OR p.description = '–û–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞')
           OR NOT EXISTS (SELECT 1 FROM specifications s WHERE s.item_id = p.item_id)
        ORDER BY p.item_id
        LIMIT 500
    ''')
    
    items = cursor.fetchall()
    conn.close()
    
    if not items:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤ –±–µ–∑ –æ–ø–∏—Å–∞–Ω–∏–π –∏–ª–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫!")
        return
    
    print(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(items)} —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –¥–æ–ø–∞—Ä—Å–∏–Ω–≥–∞")
    print(f"üìä –ü–µ—Ä–≤—ã–µ 5: {[item[0] for item in items[:5]]}")
    print()
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—Å–µ—Ä
    parser = OTAPIParser()
    
    # –ü–∞—Ä—Å–∏–º —Ç–æ–≤–∞—Ä—ã
    start_time = time.time()
    results = []
    errors = 0
    
    for i, (item_id, title, chunk_source, chunk_type) in enumerate(items, 1):
        try:
            print(f"üîç [{i}/{len(items)}] –ü–∞—Ä—Å–∏–Ω–≥ —Ç–æ–≤–∞—Ä–∞ {item_id}...")
            
            # –ü–∞—Ä—Å–∏–º —Ç–æ–≤–∞—Ä —á–µ—Ä–µ–∑ OTAPI
            result = parser.parse_product(item_id)
            
            if result:
                raw_data = result.get('raw_data', {})
                item_data = raw_data.get('Item', {})
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏
                description = item_data.get('Description', '')
                attributes = item_data.get('Attributes', [])
                physical = item_data.get('PhysicalParameters', {})
                weight_info = item_data.get('ActualWeightInfo', {})
                pictures = item_data.get('Pictures', [])
                main_image = item_data.get('MainPictureUrl', '')
                
                has_description = bool(description and description.strip())
                has_attributes = bool(attributes and len(attributes) > 0)
                has_physical = bool(physical and len(physical) > 0)
                has_weight = bool(weight_info and len(weight_info) > 0)
                has_images = bool((pictures and len(pictures) > 0) or main_image)
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                parsed_item = {
                    'id': f'abb-{item_id}',
                    'item_id': item_id,
                    'url': f'https://detail.1688.com/offer/{item_id}.html',
                    'title': title,
                    'status': 'success',
                    'raw_data': {
                        'Item': item_data
                    }
                }
                
                results.append(parsed_item)
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏
                print(f"    ‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω:")
                print(f"       üìù –û–ø–∏—Å–∞–Ω–∏–µ: {'–ï–°–¢–¨' if has_description else '–ù–ï–¢'} ({len(description)} —Å–∏–º–≤–æ–ª–æ–≤)")
                print(f"       üìã –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏: {'–ï–°–¢–¨' if has_attributes else '–ù–ï–¢'} ({len(attributes)} —à—Ç)")
                print(f"       üìè –§–∏–∑–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {'–ï–°–¢–¨' if has_physical else '–ù–ï–¢'} ({len(physical)} —à—Ç)")
                print(f"       ‚öñÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–µ—Å–µ: {'–ï–°–¢–¨' if has_weight else '–ù–ï–¢'} ({len(weight_info)} —à—Ç)")
                print(f"       üñºÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {'–ï–°–¢–¨' if has_images else '–ù–ï–¢'} ({len(pictures) + (1 if main_image else 0)} —à—Ç)")
                
            else:
                print(f"    ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
                errors += 1
            
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            if i < len(items):
                time.sleep(0.5)
                
        except Exception as e:
            print(f"    ‚ùå –û—à–∏–±–∫–∞: {e}")
            errors += 1
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 50 —Ç–æ–≤–∞—Ä–æ–≤
        if i % 50 == 0:
            print(f"\nüìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{len(items)} ({i/len(items)*100:.1f}%)")
            print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ: {len(results)}")
            print(f"   ‚ùå –û—à–∏–±–æ–∫: {errors}")
            print()
    
    end_time = time.time()
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    parsing_time = end_time - start_time
    success_rate = len(results) / len(items) * 100 if items else 0
    
    print(f"\n‚è±Ô∏è  –í–†–ï–ú–Ø –ü–ê–†–°–ò–ù–ì–ê:")
    print(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è: {parsing_time:.1f} —Å–µ–∫—É–Ω–¥")
    print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —Ç–æ–≤–∞—Ä: {parsing_time/len(items):.1f} —Å–µ–∫—É–Ω–¥")
    print(f"   –°–∫–æ—Ä–æ—Å—Ç—å: {len(results)/parsing_time:.2f} —Ç–æ–≤–∞—Ä–æ–≤/—Å–µ–∫")
    
    print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"   –í—Å–µ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(items)}")
    print(f"   –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ: {len(results)}")
    print(f"   –û—à–∏–±–æ–∫: {errors}")
    print(f"   –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {success_rate:.1f}%")
    
    if results:
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
        analyze_data_quality(results)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ chunk
        chunk_file = save_chunk(results, 3)
        
        if chunk_file:
            print(f"\n‚úÖ –ü–ê–†–°–ò–ù–ì –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù!")
            print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {chunk_file.name}")
            print(f"üìä –í—Å–µ–≥–æ —Å–ø–∞—Ä—Å–µ–Ω–æ: {len(results)} —Ç–æ–≤–∞—Ä–æ–≤")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –¥–µ–ª–∞—Ç—å –¥–∞–ª—å—à–µ
            show_next_steps()
        else:
            print(f"\n‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤!")
    else:
        print(f"\n‚ùå –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –±–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤!")

def analyze_data_quality(results):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    print(f"\nüîç –ê–ù–ê–õ–ò–ó –ö–ê–ß–ï–°–¢–í–ê –î–ê–ù–ù–´–•:")
    print("-" * 40)
    
    if not results:
        return
    
    total_items = len(results)
    items_with_description = 0
    items_with_attributes = 0
    items_with_physical = 0
    items_with_weight = 0
    items_with_images = 0
    
    total_attributes = 0
    total_physical_params = 0
    total_weight_params = 0
    total_images = 0
    total_description_length = 0
    
    for item in results:
        raw_data = item.get('raw_data', {})
        item_data = raw_data.get('Item', {})
        
        # –û–ø–∏—Å–∞–Ω–∏—è
        description = item_data.get('Description', '')
        if description and description.strip():
            items_with_description += 1
            total_description_length += len(description)
        
        # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        attributes = item_data.get('Attributes', [])
        if attributes and len(attributes) > 0:
            items_with_attributes += 1
            total_attributes += len(attributes)
        
        # –§–∏–∑–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        physical = item_data.get('PhysicalParameters', {})
        if physical and len(physical) > 0:
            items_with_physical += 1
            total_physical_params += len(physical)
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–µ—Å–µ
        weight_info = item_data.get('ActualWeightInfo', {})
        if weight_info and len(weight_info) > 0:
            items_with_weight += 1
            total_weight_params += len(weight_info)
        
        # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        pictures = item_data.get('Pictures', [])
        main_image = item_data.get('MainPictureUrl', '')
        if (pictures and len(pictures) > 0) or main_image:
            items_with_images += 1
            total_images += len(pictures) + (1 if main_image else 0)
    
    print(f"üìù –û–ø–∏—Å–∞–Ω–∏—è: {items_with_description}/{total_items} ({items_with_description/total_items*100:.1f}%)")
    if items_with_description > 0:
        avg_desc_length = total_description_length / items_with_description
        print(f"   –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {avg_desc_length:.1f} —Å–∏–º–≤–æ–ª–æ–≤")
    
    print(f"üìã –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏: {items_with_attributes}/{total_items} ({items_with_attributes/total_items*100:.1f}%)")
    if items_with_attributes > 0:
        avg_attrs = total_attributes / items_with_attributes
        print(f"   –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {avg_attrs:.1f} —à—Ç")
    
    print(f"üìè –§–∏–∑–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {items_with_physical}/{total_items} ({items_with_physical/total_items*100:.1f}%)")
    if items_with_physical > 0:
        avg_physical = total_physical_params / items_with_physical
        print(f"   –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {avg_physical:.1f} —à—Ç")
    
    print(f"‚öñÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–µ—Å–µ: {items_with_weight}/{total_items} ({items_with_weight/total_items*100:.1f}%)")
    if items_with_weight > 0:
        avg_weight = total_weight_params / items_with_weight
        print(f"   –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {avg_weight:.1f} —à—Ç")
    
    print(f"üñºÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {items_with_images}/{total_items} ({items_with_images/total_items*100:.1f}%)")
    if items_with_images > 0:
        avg_images = total_images / items_with_images
        print(f"   –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {avg_images:.1f} —à—Ç")

def save_chunk(results, chunk_number):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç chunk —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏"""
    try:
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ—Ç
        chunks_dir = Path("data/results/chunks")
        chunks_dir.mkdir(parents=True, exist_ok=True)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"otapi_chunk_{chunk_number:03d}_{timestamp}.json"
        filepath = chunks_dir / filename
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ Chunk —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename}")
        return filepath
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è chunk: {e}")
        return None

def show_next_steps():
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —á—Ç–æ –¥–µ–ª–∞—Ç—å –¥–∞–ª—å—à–µ"""
    print(f"\nüéØ –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
    print(f"   1. –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –Ω–æ–≤—ã–π chunk –≤ –ë–î")
    print(f"   2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è")
    print(f"   3. –ï—Å–ª–∏ –Ω—É–∂–Ω–æ - –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤")
    print(f"   4. –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª–Ω–æ–π –ë–î")
    
    print(f"\nüí° –ö–û–ú–ê–ù–î–´ –î–õ–Ø –í–´–ü–û–õ–ù–ï–ù–ò–Ø:")
    print(f"   # –û–±—ä–µ–¥–∏–Ω–∏—Ç—å chunk –≤ –ë–î:")
    print(f"   python3 merge_otapi_chunk.py")
    print(f"   ")
    print(f"   # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ë–î:")
    print(f"   python3 check_final_database.py")

if __name__ == "__main__":
    parse_missing_data()
