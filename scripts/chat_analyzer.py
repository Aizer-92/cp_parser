#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —á–∞—Ç–æ–≤ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ –¥–∏–∞–ª–æ–≥–æ–≤ –∏–∑ ChatGPT/Perplexity

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python scripts/chat_analyzer.py "—Ç–µ–∫—Å—Ç —á–∞—Ç–∞"
    python scripts/chat_analyzer.py --file "–ø—É—Ç—å_–∫_—Ñ–∞–π–ª—É.txt"
"""

import argparse
import re
import json
from datetime import datetime
from pathlib import Path

# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏
CATEGORY_KEYWORDS = {
    'Business': [
        'startup', 'business', '–ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å—Å—Ç–≤–æ', '–±–∏–∑–Ω–µ—Å', '–∫–æ–º–ø–∞–Ω–∏—è', '—Å—Ç—Ä–∞—Ç–µ–≥–∏—è',
        '–º–∞—Ä–∫–µ—Ç–∏–Ω–≥', '–ø—Ä–æ–¥–∞–∂–∏', '–∫–ª–∏–µ–Ω—Ç—ã', 'revenue', 'profit', '–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã',
        'scaling', 'growth', 'venture', 'investment round', 'pitch', 'business plan'
    ],
    'Health': [
        '–∑–¥–æ—Ä–æ–≤—å–µ', 'health', '–º–µ–¥–∏—Ü–∏–Ω–∞', 'nutrition', '–ø–∏—Ç–∞–Ω–∏–µ', '–¥–∏–µ—Ç–∞', 'fitness',
        '—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞', 'exercise', 'weight', '–≤–µ—Å', '–±–æ–ª–µ–∑–Ω—å', '—Å–∏–º–ø—Ç–æ–º—ã', '–≤—Ä–∞—á',
        '–ª–µ—á–µ–Ω–∏–µ', '–ø—Ä–µ–ø–∞—Ä–∞—Ç', '–≤–∏—Ç–∞–º–∏–Ω—ã', 'wellness', 'mental health', '–∞–Ω–∞–ª–∏–∑—ã',
        '—Ç–µ—Å—Ç–æ—Å—Ç–µ—Ä–æ–Ω', 'testosterone', '–≥–æ—Ä–º–æ–Ω—ã', 'hormones', '—ç—Å—Ç—Ä–∞–¥–∏–æ–ª', 'estradiol',
        '—Ñ—Å–≥', 'fsh', '–ª–≥', 'lh', '–≥—Å–ø–≥', 'shbg', '—ç–Ω–¥–æ–∫—Ä–∏–Ω–æ–ª–æ–≥–∏—è', 'endocrinology'
    ],
    'Finance': [
        '—Ñ–∏–Ω–∞–Ω—Å—ã', 'finance', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏', 'investment', '–∞–∫—Ü–∏–∏', 'stocks', 'crypto',
        '–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞', 'portfolio', '–ø–æ—Ä—Ç—Ñ–µ–ª—å', 'budget', '–±—é–¥–∂–µ—Ç', 'savings',
        '–Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è', 'debt', '–¥–æ–ª–≥', 'pension', '–ø–µ–Ω—Å–∏—è', 'tax', '–Ω–∞–ª–æ–≥–∏'
    ],
    'Technology': [
        '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ', 'programming', 'python', 'javascript', 'code', '–∫–æ–¥',
        'algorithm', '–∞–ª–≥–æ—Ä–∏—Ç–º', 'API', 'database', 'frontend', 'backend',
        'AI', 'artificial intelligence', 'machine learning', 'data science'
    ],
    'Learning': [
        '–æ–±—É—á–µ–Ω–∏–µ', 'learning', 'education', '–∫—É—Ä—Å', 'course', 'study', '–∏–∑—É—á–µ–Ω–∏–µ',
        '–Ω–∞–≤—ã–∫', 'skill', '—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è', 'certification', '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç', 'university',
        '–∫–Ω–∏–≥–∞', 'book', 'tutorial', '—É—Ä–æ–∫', 'lesson', 'knowledge'
    ],
    'Home': [
        '–¥–æ–º', 'home', '—Ä–µ–º–æ–Ω—Ç', 'renovation', 'design', '–¥–∏–∑–∞–π–Ω', '–º–µ–±–µ–ª—å', 'furniture',
        '–∫—É—Ö–Ω—è', 'kitchen', '–≤–∞–Ω–Ω–∞—è', 'bathroom', 'garden', '—Å–∞–¥', 'smart home',
        '—É–º–Ω—ã–π –¥–æ–º', '–ø—Ä–æ–µ–∫—Ç', 'project', 'DIY'
    ],
    'Work': [
        '—Ä–∞–±–æ—Ç–∞', 'work', '–∫–∞—Ä—å–µ—Ä–∞', 'career', 'job', '–¥–æ–ª–∂–Ω–æ—Å—Ç—å', 'position',
        'resume', '—Ä–µ–∑—é–º–µ', 'interview', '—Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ', 'salary', '–∑–∞—Ä–ø–ª–∞—Ç–∞',
        'promotion', '–ø–æ–≤—ã—à–µ–Ω–∏–µ', 'team', '–∫–æ–º–∞–Ω–¥–∞', 'manager', '–º–µ–Ω–µ–¥–∂–µ—Ä'
    ]
}

# –¢–µ–≥–∏ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
COMMON_TAGS = {
    'startup', 'ai', 'python', 'javascript', 'react', 'health', 'fitness',
    'investment', 'crypto', 'marketing', 'design', 'productivity', 'learning',
    'testosterone', 'hormones', 'endocrinology', 'blood_test', 'medical'
}

def analyze_text_category(text):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é"""
    text_lower = text.lower()
    category_scores = {}
    
    for category, keywords in CATEGORY_KEYWORDS.items():
        score = 0
        for keyword in keywords:
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
            score += len(re.findall(r'\b' + re.escape(keyword.lower()) + r'\b', text_lower))
        category_scores[category] = score
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Å –Ω–∞–∏–≤—ã—Å—à–∏–º —Å—á–µ—Ç–æ–º
    if max(category_scores.values()) > 0:
        return max(category_scores, key=category_scores.get)
    else:
        return 'Personal'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é

def extract_tags(text):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ç–µ–≥–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    text_lower = text.lower()
    found_tags = []
    
    for tag in COMMON_TAGS:
        if tag in text_lower:
            found_tags.append(f'#{tag}')
    
    return found_tags[:5]  # –ú–∞–∫—Å–∏–º—É–º 5 —Ç–µ–≥–æ–≤

def extract_key_insights(text):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞ —á–∞—Ç–∞"""
    insights = []
    
    # –ò—â–µ–º —Ñ—Ä–∞–∑—ã, –∫–æ—Ç–æ—Ä—ã–µ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –≤—ã–≤–æ–¥—ã –∏–ª–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    patterns = [
        r'—Ä–µ–∫–æ–º–µ–Ω–¥—É—é[^.]*[.]',
        r'–≤–∞–∂–Ω–æ[^.]*[.]',
        r'–∫–ª—é—á–µ–≤–æ–π[^.]*[.]',
        r'–≥–ª–∞–≤–Ω–æ–µ[^.]*[.]',
        r'–≤—ã–≤–æ–¥[^.]*[.]',
        r'recommend[^.]*[.]',
        r'important[^.]*[.]',
        r'key[^.]*[.]',
        r'conclusion[^.]*[.]'
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        insights.extend(matches[:3])  # –ú–∞–∫—Å–∏–º—É–º 3 –∏–Ω—Å–∞–π—Ç–∞ –Ω–∞ –ø–∞—Ç—Ç–µ—Ä–Ω
    
    return insights[:5]  # –ú–∞–∫—Å–∏–º—É–º 5 –∏–Ω—Å–∞–π—Ç–æ–≤

def suggest_filename(text, category):
    """–ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∏–º—è —Ñ–∞–π–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"""
    today = datetime.now().strftime("%Y-%m-%d")
    
    # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—É—é —Ç–µ–º—É –∏–∑ –ø–µ—Ä–≤—ã—Ö 200 —Å–∏–º–≤–æ–ª–æ–≤
    first_part = text[:200].lower()
    
    # –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º—ã
    topic_keywords = {
        '–ø–ª–∞–Ω': 'Plan',
        '—Å—Ç—Ä–∞—Ç–µ–≥–∏—è': 'Strategy', 
        '–∞–Ω–∞–ª–∏–∑': 'Analysis',
        '—Å–æ–∑–¥–∞–Ω–∏–µ': 'Creation',
        '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞': 'Development',
        '–∏–∑—É—á–µ–Ω–∏–µ': 'Learning',
        '—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ': 'Comparison',
        '–ø–ª–∞–Ω': 'Planning'
    }
    
    topic = "Discussion"
    for keyword, english in topic_keywords.items():
        if keyword in first_part:
            topic = english
            break
    
    return f"{today}_{category}_{topic}"

def create_markdown_template(text, category, tags, insights, filename):
    """–°–æ–∑–¥–∞–µ—Ç markdown-—à–∞–±–ª–æ–Ω –¥–ª—è —á–∞—Ç–∞"""
    
    template = f"""# {filename.replace('_', ' ').replace('-', '.')}

**–î–∞—Ç–∞:** {datetime.now().strftime("%d.%m.%Y")}  
**–ò—Å—Ç–æ—á–Ω–∏–∫:** [ChatGPT/Perplexity]  
**–ö–∞—Ç–µ–≥–æ—Ä–∏—è:** {category}  
**–¢–µ–≥–∏:** {' '.join(tags) if tags else '#–Ω–æ–≤—ã–π_—á–∞—Ç'}

## üéØ –û —á–µ–º –¥–∏–∞–ª–æ–≥
[–û–ø–∏—à–∏—Ç–µ —Å—É—Ç—å –∏ —Ü–µ–ª—å —Ä–∞–∑–≥–æ–≤–æ—Ä–∞]

## üí° –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã
"""
    
    if insights:
        for insight in insights:
            template += f"- {insight.strip()}\n"
    else:
        template += "- [–ö–ª—é—á–µ–≤–æ–π –≤—ã–≤–æ–¥ 1]\n- [–ö–ª—é—á–µ–≤–æ–π –≤—ã–≤–æ–¥ 2]\n- [–ö–ª—é—á–µ–≤–æ–π –≤—ã–≤–æ–¥ 3]\n"
    
    template += """
## ‚úÖ –ß—Ç–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å
- [ ] [–ö–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ 1]
- [ ] [–ö–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ 2]
- [ ] [–ö–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ 3]

---

## üí¨ –î–∏–∞–ª–æ–≥

"""
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Å–∞–º —Ç–µ–∫—Å—Ç —á–∞—Ç–∞
    template += text
    
    template += """

---

## üîó –°–≤—è–∑–∞–Ω–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
- [–°—Å—ã–ª–∫–∏ –Ω–∞ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ –≤–∞—à–µ–π —Å–∏—Å—Ç–µ–º–µ]

## üìù –ú–æ–∏ –∑–∞–º–µ—Ç–∫–∏
[–í–∞—à–∏ –º—ã—Å–ª–∏ –ø–æ—Å–ª–µ –¥–∏–∞–ª–æ–≥–∞, –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–¥–µ–∏]
"""
    
    return template

def main():
    parser = argparse.ArgumentParser(description='–ê–Ω–∞–ª–∏–∑ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –ò–ò-—á–∞—Ç–æ–≤')
    parser.add_argument('text', nargs='?', help='–¢–µ–∫—Å—Ç —á–∞—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞')
    parser.add_argument('--file', help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —Ç–µ–∫—Å—Ç–æ–º —á–∞—Ç–∞')
    parser.add_argument('--output', help='–ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞')
    
    args = parser.parse_args()
    
    if args.file:
        try:
            with open(args.file, 'r', encoding='utf-8') as f:
                text = f.read()
        except FileNotFoundError:
            print(f"‚ùå –§–∞–π–ª {args.file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return
    elif args.text:
        text = args.text
    else:
        print("‚ùå –£–∫–∞–∂–∏—Ç–µ —Ç–µ–∫—Å—Ç —á–∞—Ç–∞ –∏–ª–∏ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É")
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python chat_analyzer.py '—Ç–µ–∫—Å—Ç' –∏–ª–∏ --file file.txt")
        return
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
    print("üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —á–∞—Ç...")
    category = analyze_text_category(text)
    tags = extract_tags(text)
    insights = extract_key_insights(text)
    filename = suggest_filename(text, category)
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞:")
    print(f"üìÅ –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}")
    print(f"üè∑Ô∏è  –¢–µ–≥–∏: {', '.join(tags) if tags else '–ù–µ –Ω–∞–π–¥–µ–Ω–æ'}")
    print(f"üí° –ò–Ω—Å–∞–π—Ç–æ–≤ –Ω–∞–π–¥–µ–Ω–æ: {len(insights)}")
    print(f"üìù –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞: {filename}.md")
    
    # –°–æ–∑–¥–∞–µ–º —à–∞–±–ª–æ–Ω
    markdown_content = create_markdown_template(text, category, tags, insights, filename)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞ –ø–∞–ø–∫–∞ –≤—ã–≤–æ–¥–∞
    if args.output:
        output_dir = Path(args.output)
        output_dir.mkdir(parents=True, exist_ok=True)
        output_file = output_dir / f"{filename}.md"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        print(f"‚úÖ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")
    else:
        print(f"\nüìÑ Markdown —à–∞–±–ª–æ–Ω:")
        print("=" * 50)
        print(markdown_content[:500] + "..." if len(markdown_content) > 500 else markdown_content)

if __name__ == "__main__":
    main()
